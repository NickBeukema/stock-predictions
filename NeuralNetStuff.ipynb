{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2363.61 2364.10 2362.68 2364.31]\n",
      " [2364.10 2362.68 2364.31 2364.85]\n",
      " [2362.68 2364.31 2364.85 2365.62]\n",
      " [2364.31 2364.85 2365.62 2365.20]\n",
      " [2364.85 2365.62 2365.20 2365.29]\n",
      " [2365.62 2365.20 2365.29 2364.32]\n",
      " [2365.20 2365.29 2364.32 2364.64]\n",
      " [2365.29 2364.32 2364.64 2364.58]\n",
      " [2364.32 2364.64 2364.58 2364.48]\n",
      " [2364.64 2364.58 2364.48 2365.23]]\n",
      "\n",
      "\n",
      "Saved data to 'output.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "# Change these to switch around parameters\n",
    "#\n",
    "\n",
    "set_size = 4 # Number of consecutive days in a row\n",
    "data_column = 1 # Index of column to use (S&P - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data from current directory\n",
    "data = pd.read_csv('./data_stocks.csv')\n",
    "\n",
    "data = data.values\n",
    "\n",
    "# Pull specified column of data\n",
    "snp = data[:,np.arange(data_column, data_column + 1)]\n",
    "\n",
    "data_set = []\n",
    "\n",
    "# Loop over full set\n",
    "for idx in range(len(snp)):\n",
    "\n",
    "    row = []\n",
    "\n",
    "    # Check if our range goes over the list length\n",
    "    if idx + set_size > len(snp):\n",
    "        break\n",
    "\n",
    "    # Build a row starting at current\n",
    "    # index to index + set_size\n",
    "    for v in range(set_size):\n",
    "        is_last = v == (set_size - 1)\n",
    "        \n",
    "        current_index = idx + v\n",
    "        current_value = snp[current_index][0]\n",
    "\n",
    "        # Produce last day predictor if it's the last\n",
    "        # iteration\n",
    "#         if is_last:\n",
    "#             last_value = row[-1]\n",
    "#             diff = current_value - last_value\n",
    "            \n",
    "#             # Reults in either [-1,0,1]\n",
    "#             predictor = 0 if diff == 0 else (diff/abs(diff))\n",
    "#             row.append(predictor)\n",
    "            \n",
    "#         # Otherwise append the value\n",
    "#         else:\n",
    "#             row.append(current_value)\n",
    "\n",
    "        row.append(current_value)\n",
    "\n",
    "\n",
    "    row = np.array(row)\n",
    "    data_set.append(row)\n",
    "\n",
    "\n",
    "data_set = np.asarray(data_set)\n",
    "\n",
    "fmt = {'float_kind':'{:0.2f}'.format}\n",
    "np.set_printoptions(suppress=True, formatter=fmt)\n",
    "\n",
    "# Prints top 10 records for quick confirmation\n",
    "print(data_set[0:10])\n",
    "\n",
    "np.savetxt(\"output.csv\", data_set, delimiter=\",\", fmt=\"%10.2f\")\n",
    "print(\"\\n\\nSaved data to 'output.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.136142\n",
      "0.238541\n",
      "0.132395\n",
      "0.238559\n",
      "0.133501\n",
      "0.238555\n",
      "0.132549\n",
      "0.238457\n",
      "0.132662\n",
      "0.238521\n",
      "0.131925\n",
      "0.238652\n",
      "0.132038\n",
      "0.238636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-50b7840413ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = pd.read_csv('./output.csv')\n",
    "\n",
    "data = data.values\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split data into inputs and outputs\n",
    "# inputs = data[:, np.arange(0,2)]\n",
    "# outputs = data[:, 3]\n",
    "\n",
    "# Scale Data to [-1,1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaled_input = scaler.fit_transform(inputs)\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "# Split data into inputs and outputs\n",
    "inputs = scaled_data[:, np.arange(0,2)]\n",
    "outputs = scaled_data[:, 3]\n",
    "\n",
    "# Split Data into 80% train, 20% test\n",
    "cutoff = int(np.floor(0.8 * len(scaled_input)))\n",
    "\n",
    "train_inputs = scaled_input[:cutoff]\n",
    "train_outputs = outputs[:cutoff]\n",
    "\n",
    "test_inputs = scaled_input[cutoff + 1:]\n",
    "test_outputs = outputs[cutoff + 1:]\n",
    "\n",
    "# Setup placeholders for input and output\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None])\n",
    "            \n",
    "# Build basic network with sigmoid activation\n",
    "# and using the pre-built dense layer\n",
    "network = tf.layers.dense(X, units=1, activation=tf.nn.sigmoid)\n",
    "network = tf.layers.dense(network, units=1, activation=tf.nn.sigmoid)\n",
    "cost = tf.reduce_mean((network - Y)**2)\n",
    "# cost = tf.reduce_mean(tf.squared_difference(network, Y))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "   \n",
    "    for epoch in range(100):\n",
    "        shuffled_indices = np.random.permutation(np.arange(len(train_inputs)))\n",
    "        train_inputs = train_inputs[shuffled_indices]\n",
    "        train_outputs = train_outputs[shuffled_indices]\n",
    "        \n",
    "        for i in range(len(train_outputs // batch_size)):\n",
    "            start = i * batch_size\n",
    "            batch_x = train_inputs[start:start + batch_size]\n",
    "            batch_y = train_outputs[start:start + batch_size]\n",
    "            \n",
    "            sess.run([optimizer, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "   \n",
    "        print(sess.run(cost, feed_dict={X: train_inputs, Y: train_outputs}))\n",
    "        print(sess.run(cost, feed_dict={X: test_inputs, Y: test_outputs}))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29ead0afee84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#lstm = tf.contrib.rnn.BasicLSTMCell(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Tensor'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.add(1, 2, name=\"Add_these_numbers\")\n",
    "b = tf.multiply(a, 3)\n",
    "c = tf.add(4, 5, name=\"And_These_ones\")\n",
    "d = tf.multiply(c, 6, name=\"Multiply_these_numbers\")\n",
    "e = tf.multiply(4, 5, name=\"B_add\")\n",
    "f = tf.div(c, 6, name=\"B_mul\")\n",
    "g = tf.add(b, d)\n",
    "h = tf.multiply(g, f)\n",
    "s = tf.sigmoid(float(g))\n",
    "#lstm = tf.contrib.rnn.BasicLSTMCell(5)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(s))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    print(sess.run(h))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('./project3/data_stocks.csv')\n",
    "print (data[1])\n",
    "data = data.values\n",
    "print (data[1])\n",
    "n = data.shape[0]\n",
    "p = data.shape[1]\n",
    "\n",
    "pyplot.plot(data['SP500'])\n",
    "\n",
    "#ten previous prices\n",
    "sequence_length = 10\n",
    "\n",
    "input_dimension = 1\n",
    "data = tf.placeholder(tf.float32, [None, 20,1]) #Number of examples, number of input, dimension of each input\n",
    "target = tf.placeholder(tf.float32, [None, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[-1.0, -0.3333333320915699, 0.33333333395421505, 1.0]\n",
      "[-1.0, -0.3333333320915699, 0.33333333395421505]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20026046]\n",
      " [-0.82043386]\n",
      " [-1.1406693 ]\n",
      " [-1.4609053 ]]\n",
      "39.273277\n",
      "1.9885426\n",
      "0.39249957\n",
      "0.31693798\n",
      "0.3063346\n",
      "0.29868105\n",
      "0.29133132\n",
      "0.2841685\n",
      "0.27718318\n",
      "0.2703708\n",
      "0.26372698\n",
      "0.2572477\n",
      "0.25092882\n",
      "0.24476631\n",
      "0.23875639\n",
      "0.23289524\n",
      "0.22717918\n",
      "0.2216045\n",
      "0.21616793\n",
      "0.21086589\n",
      "0.20569518\n",
      "0.20065235\n",
      "0.1957343\n",
      "0.19093822\n",
      "0.18626066\n",
      "0.18169905\n",
      "0.17725019\n",
      "0.17291155\n",
      "0.16868028\n",
      "0.16455378\n",
      "0.1605293\n",
      "0.15660457\n",
      "0.15277694\n",
      "0.14904407\n",
      "0.1454036\n",
      "0.14185323\n",
      "0.13839075\n",
      "0.1350139\n",
      "0.13172072\n",
      "0.128509\n",
      "0.12537685\n",
      "0.12232228\n",
      "0.11934315\n",
      "0.116437845\n",
      "0.113604546\n",
      "0.11084127\n",
      "0.10814643\n",
      "0.105518185\n",
      "0.1029551\n",
      "0.10045555\n",
      "0.09801775\n",
      "0.09564029\n",
      "0.093321726\n",
      "0.09106047\n",
      "0.0888554\n",
      "0.086704664\n",
      "0.08460732\n",
      "0.082561746\n",
      "0.08056692\n",
      "0.078621395\n",
      "0.07672408\n",
      "0.07487373\n",
      "0.07306916\n",
      "0.071309224\n",
      "0.06959289\n",
      "0.06791901\n",
      "0.06628662\n",
      "0.06469467\n",
      "0.063142\n",
      "0.061627768\n",
      "0.06015113\n",
      "0.058710936\n",
      "0.05730642\n",
      "0.05593663\n",
      "0.054600883\n",
      "0.0532981\n",
      "0.052027464\n",
      "0.050788417\n",
      "0.04958002\n",
      "0.048401378\n",
      "0.047252078\n",
      "0.04613127\n",
      "0.045038097\n",
      "0.043971907\n",
      "0.042932253\n",
      "0.041918267\n",
      "0.04092938\n",
      "0.039964985\n",
      "0.039024472\n",
      "0.038107164\n",
      "0.03721263\n",
      "0.036340144\n",
      "0.035489403\n",
      "0.034659624\n",
      "0.03385037\n",
      "0.033061136\n",
      "0.032291487\n",
      "0.031540807\n",
      "0.030808726\n",
      "0.030094797\n",
      "[[0.72412133]\n",
      " [4.834022  ]\n",
      " [5.9746623 ]\n",
      " [7.1153016 ]]\n",
      "[2362.6799 2364.8501]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#scaler.fit(data)\n",
    "\n",
    "#data = pd.read_csv('./project3/data_stocks.csv')\n",
    "#print (data['SP500'][0:100])\n",
    "#data = data.values\n",
    "#print (data)\n",
    "#n = data.shape[0]\n",
    "#p = data.shape[1]\n",
    "\n",
    "#print (data[2])\n",
    "\n",
    "\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#scaler.fit(data['SP500'])\n",
    "days = [1491226740.00,1491226800.00,1491226860.00,1491226920.00]\n",
    "#days = [2363.6101, 2364.1001, 2362.6799, 2364.8501]\n",
    "print (type(days))\n",
    "scaler.fit(days)\n",
    "days = scaler.transform(days)\n",
    "print (type(list(days)))\n",
    "days = list(days)\n",
    "print (days)\n",
    "day2 = days[0:3]\n",
    "print (day2)\n",
    "print (days[3])\n",
    "day3 = days[3]\n",
    "x = tf.constant([day2, [2, 3, 4], [3, 4, 5], [4, 5, 6]], dtype=tf.float32)\n",
    "y_true = tf.constant([[day3], [5], [6], [7]], dtype=tf.float32)\n",
    "\n",
    "previous = tf.constant([[0, 0,0,0]],dtype=tf.float32)\n",
    "tomorrow = tf.constant([[0]], dtype=tf.float32)\n",
    "\n",
    "\n",
    "#x = tf.constant([[2363.6101], [2364.1001], [2362.6799], [2364.3101]], dtype=tf.float32)\n",
    "#y_true = tf.constant([[2364.1001], [2362.6799], [2364.3101], [2364.8501]], dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#yesterday = data['SP500']\n",
    "#today = data['SP500'][1:6]\n",
    "#print (yesterday)\n",
    "#print (yesterday)\n",
    "\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "\n",
    "y_pred = linear_model(x)\n",
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print (sess.run(y_pred))\n",
    "for i in range(100):\n",
    "  _, loss_value = sess.run((train, loss))\n",
    "  #print (sess.run(y_pred))\n",
    "  print(loss_value)\n",
    "\n",
    "print(sess.run(y_pred))\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(2362.6799, 2364.8501))\n",
    "thing = [0.11891884, .3]\n",
    "scaler.fit(thing)\n",
    "thing = scaler.transform(thing)\n",
    "print (thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = tf.contrib.rnn.BasicLSTMCell(5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    print(sess.run(lstm))\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "day1 = tf.constant(100, dtype=tf.float32)\n",
    "day2 = tf.constant(150, dtype=tf.float32)\n",
    "s = tf.sigmoid(day1)\n",
    "y_true = tf.constant(200, dtype=tf.float32)\n",
    "sess = tf.Session()\n",
    "print (sess.run(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
