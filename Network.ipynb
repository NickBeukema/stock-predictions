{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Formatting\n",
    "\n",
    "Run this first cell to generate the data necessary for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42.33 42.36 42.31 42.37 42.54]\n",
      " [42.36 42.31 42.37 42.54 42.54]\n",
      " [42.31 42.37 42.54 42.54 42.47]\n",
      " [42.37 42.54 42.54 42.47 42.47]\n",
      " [42.54 42.54 42.47 42.47 42.39]\n",
      " [42.54 42.47 42.47 42.39 42.33]\n",
      " [42.47 42.47 42.39 42.33 42.40]\n",
      " [42.47 42.39 42.33 42.40 42.29]\n",
      " [42.39 42.33 42.40 42.29 42.29]\n",
      " [42.33 42.40 42.29 42.29 42.39]]\n",
      "\n",
      "\n",
      "Saved data to 'output.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "# Change these to switch around parameters\n",
    "#\n",
    "\n",
    "set_size = 5 # Number of consecutive days in a row\n",
    "data_column = 1 # Index of column to use (S&P - 1)\n",
    "\n",
    "\n",
    "# Load data from current directory\n",
    "data = pd.read_csv('./data_stocks.csv')\n",
    "\n",
    "data = data.values\n",
    "\n",
    "# Pull specified column of data\n",
    "snp = data[:,np.arange(data_column, data_column + 1)]\n",
    "\n",
    "data_set = []\n",
    "\n",
    "# Loop over full set\n",
    "for idx in range(len(snp)):\n",
    "\n",
    "    row = []\n",
    "\n",
    "    # Check if our range goes over the list length\n",
    "    if idx + set_size > len(snp):\n",
    "        break\n",
    "\n",
    "    # Build a row starting at current\n",
    "    # index to index + set_size\n",
    "    for v in range(set_size):\n",
    "        is_last = v == (set_size - 1)\n",
    "        \n",
    "        current_index = idx + v\n",
    "        current_value = snp[current_index][0]\n",
    "\n",
    "        # Produce last day predictor if it's the last\n",
    "        # iteration\n",
    "#         if is_last:\n",
    "#             last_value = row[-1]\n",
    "#             diff = current_value - last_value\n",
    "            \n",
    "#             # Reults in either [-1,0,1]\n",
    "#             predictor = 0 if diff == 0 else (diff/abs(diff))\n",
    "#             row.append(predictor)\n",
    "            \n",
    "#         # Otherwise append the value\n",
    "#         else:\n",
    "#             row.append(current_value)\n",
    "\n",
    "        row.append(current_value)\n",
    "\n",
    "\n",
    "    row = np.array(row)\n",
    "    data_set.append(row)\n",
    "\n",
    "\n",
    "data_set = np.asarray(data_set)\n",
    "\n",
    "fmt = {'float_kind':'{:0.2f}'.format}\n",
    "np.set_printoptions(suppress=True, formatter=fmt)\n",
    "\n",
    "# Prints top 10 records for quick confirmation\n",
    "print(data_set[0:10])\n",
    "\n",
    "np.savetxt(\"output.csv\", data_set, delimiter=\",\", fmt=\"%10.2f\")\n",
    "print(\"\\n\\nSaved data to 'output.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Train Error: 0.12543058395385742, Test Error: 0.14533810317516327\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "#\n",
    "# Change these to switch around parameters\n",
    "#\n",
    "\n",
    "number_of_days = 4\n",
    "percent_training = 0.8\n",
    "data = pd.read_csv('./output.csv')\n",
    "\n",
    "\n",
    "\n",
    "data = data.values\n",
    "np.random.shuffle(data) # Interesting to comment this out\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Split data into inputs and outputs\n",
    "# without scaling the output\n",
    "\n",
    "# inputs = data[:, np.arange(0,number_of_days - 1)]\n",
    "# outputs = data[:, number_of_days]\n",
    "# scaled_input = scaler.fit_transform(inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split data into inputs and outputs\n",
    "# also scaling the output\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_input = scaled_data[:, np.arange(0,number_of_days - 1)]\n",
    "outputs = scaled_data[:, number_of_days]\n",
    "\n",
    "\n",
    "\n",
    "# Split Data into train and test\n",
    "cutoff = int(np.floor(percent_training * len(scaled_input)))\n",
    "\n",
    "train_inputs = scaled_input[:cutoff]\n",
    "train_outputs = outputs[:cutoff]\n",
    "\n",
    "test_inputs = scaled_input[cutoff + 1:]\n",
    "test_outputs = outputs[cutoff + 1:]\n",
    "\n",
    "# Setup placeholders for input and output\n",
    "X = tf.placeholder(tf.float32, [None, number_of_days - 1])\n",
    "Y = tf.placeholder(tf.float32, [None])\n",
    "            \n",
    "# Build basic network with sigmoid activation\n",
    "# and using the pre-built dense layer\n",
    "network = tf.layers.dense(X, units=1, activation=tf.nn.sigmoid)\n",
    "network = tf.layers.dense(network, units=1, activation=tf.nn.sigmoid)\n",
    "\n",
    "cost = tf.reduce_mean(tf.squared_difference(network, Y))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Optionally use Gradient Descent Optimizer instead\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "training_error = []\n",
    "testing_error = []\n",
    "batch_array = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "   \n",
    "    for epoch in range(30):\n",
    "        shuffled_indices = np.random.permutation(np.arange(len(train_inputs)))\n",
    "        train_inputs = train_inputs[shuffled_indices]\n",
    "        train_outputs = train_outputs[shuffled_indices]\n",
    "        \n",
    "        r = len(train_outputs) // batch_size\n",
    "        if epoch == 0:\n",
    "            print(r)\n",
    "        \n",
    "        # To keep track of every epoch, uncomment below (takes quite a bit longer)\n",
    "        # If you do this, make sure to uncomment the graph at the bottom\n",
    "        \n",
    "#         train_error = sess.run(cost, feed_dict={X: train_inputs, Y: train_outputs})\n",
    "#         test_error = sess.run(cost, feed_dict={X: test_inputs, Y: test_outputs})\n",
    "#         training_error.append(train_error)\n",
    "#         testing_error.append(test_error)\n",
    "#         batch_array.append(epoch)\n",
    "        \n",
    "#         print(\"Epoch {}\".format(epoch))\n",
    "#         print(train_error)\n",
    "#         print(test_error)\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "        for i in range(r):\n",
    "            start = i * batch_size\n",
    "            batch_x = train_inputs[start:start + batch_size]\n",
    "            batch_y = train_outputs[start:start + batch_size]\n",
    "            \n",
    "            sess.run([optimizer, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "            \n",
    "    train_error = sess.run(cost, feed_dict={X: train_inputs, Y: train_outputs})\n",
    "    test_error = sess.run(cost, feed_dict={X: test_inputs, Y: test_outputs})  \n",
    "    \n",
    "    print(\"Train Error: {}, Test Error: {}\".format(train_error, test_error))\n",
    "             \n",
    "   \n",
    "# To graph progress, uncomment below (make sure data is being recorded above!)\n",
    "# plt.figure()\n",
    "# plt.title('Error Rate Over Time')\n",
    "# plt.ylabel('Error')\n",
    "# plt.xlabel('Batch')\n",
    "# plt.plot(batch_array, training_error, label='Training')\n",
    "# plt.plot(batch_array, testing_error, label='Testing')\n",
    "# plt.legend()\n",
    "# print(\"\\nTraining Error\")\n",
    "# print(training_error)\n",
    "# print(\"\\nTestng Error\")\n",
    "# print(testing_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
